
\lecture{15}{2024-11-05}{Théroème du rang}{}
\begin{parag}
    

\begin{subparag}{remarque importante}
Soit $A$ une matrice de taille $m \times n$.
\begin{theoreme}

    $\dim ColA = \dim\; LgnA$

\end{theoreme}
\begin{itemize}
    \item Le nombre de lignes linéairement indépendantes est égal au nombre de lignes contenant un pivot.
    \item Le nombre de colonnes linéairement indépendantes est égal au nombre de colonnes contenant un pivot.
    \item En résumé, $\dim\; ColA = \dim\; LgnA$ car les deux dimensions coïncident avec le nombre de pivots.

\end{itemize}
    
\end{subparag}


\begin{subparag}{exemple}

Le cas d'une matrice $2\times 2$.
Soit $A =  \begin{pmatrix}
    0 & 0 \\ 0 & 0
\end{pmatrix}$, alors $\dim\; LgnA = 0 = \dim \; ColA$.\\
Supposons que $A \neq 0$, alors $A = \begin{pmatrix}
    a & b \\ c & d
\end{pmatrix}$. Si les deux colonnes sont proportionnelles, $\dim\; ColA = 1$  On peut supposer que $\begin{pmatrix}
    b \\ d
\end{pmatrix} = \begin{pmatrix}
    \lambda a \\ \lambda c
\end{pmatrix}$ pour une $\lambda \in $\R. 
\[A = \begin{pmatrix}
    a & \lambda a \\ c & \lambda c
\end{pmatrix}\]
\textbf{Premier cas:} $a = 0$, aussi $\lambda a = 0$ et comme $A \neq 0$, $\dim \; LgnA = 1$ ça la $2^e$ ligne est non nulle.
\\
\textbf{deuxième cas} $a \neq 0$ $\begin{pmatrix}
    a & \lambda a \\ c & \lambda c
\end{pmatrix}  $ a deux lignes proportionnelles car $\left(c \; \lambda c\right) = \frac{c}{a}\left(a \; \lambda a\right)$. Ici aussi $\dim \; LgnA = 1$.
    
\end{subparag}

\end{parag}

\begin{parag}{Le Théorème du rang}
    



\paragraph{A faire plus tard parce que je sais pas}
    On se ramène au cas d'une application linéaire $R^n \to R^m$, représentée par $A \in M_{m\times n}\left(\mathbb{R} \right)$.
    \begin{enumerate}
        \item On choisit une base $\mathcal{B} = \left(e_1, \dots, e_n\right)$ de $V$
        \item On considère la composition suivante
 \item     \end{enumerate}


\begin{theoreme}{théorème du rang}
    
        \[rangT + \dim\; \kerT  = \dimV\]
    
\end{theoreme}
\begin{itemize}
    \item Grâce aux coordonnées on identifie $V$ avec \R$^n$ et W avec \R$^m$.
    \item Ainsi on identifie $T$ avec une application linéaire $S: \mathbb{N} \to \mathbb{R}^m$.
    \item Celle-ci est donnée par la multiplication matricielle \[\vec{x} \to A\vec{x} = S\left(\vec{x}\right)\]
    \item $\dim \; \ker A = $ nombre de colonnes sans pivot
    \item $\dim \; ImA = \text{rang}A = $ nombre de colonnes-pivot
    \item  nombre de colonnes-pivot $+$ colonnes sans pivot $= n$.
\end{itemize}

\begin{subparag}{Idée de la preuve}
    

Si $\dim V = n$, $\dim \; \ker T = k \leq n$ car $\ker T \subset V$. On choisit une base $\left(e_1, \dots, e_k\right)$ de $\ker T$. Grâce au Théorème de la base complétée, il existe $\left(e_{k+1}, \dots, e_n\right)$  tel que $\left(e_1, \dots, e_k, e_{k+1}, \dots, e_n\right) $ est une base de V. 
\\
Alors $\{T\left(e_1\right), \dots, T\left(e_k\right), T\left(e_{k+1}\right), \dots, T\left(e_n\right)\}$ engendrent $ImT$.
\\
Donc $\{T\left(e_{k+1}, \dots, T\left(e_n\right)\right)\}$ engendrent $ImT$.
\\
Mais T | $Vect\{e_{k+1}, \dots, e_n\}$ est injective car $\kerT \cap U = \{0\}$.
Donc $T\left(e_{k+1}\right), \dots, T\left(e_n\right)$ sont libres, donc forment une base. Conclusion \[RangT  = n - k\]
\end{subparag}


\begin{definition}
Soit $T : V \to W$ une application linéaire entre espaces vectoriels de dimension finies.
\\
Le \textcolor{red}{rang} de $T$ est la dimension de l'image de $T$ : 
\[rang T = \dim \; ImT\]
\end{definition}

\begin{definition}
Soit $A$ une matrice $m \times n$, représentant une application linéaire.
\\
    Le \textcolor{red}{rang} de $A$ est la dimension de l'image de $A: $ $rangA = \dim\; ImA$.
\end{definition}
\begin{theoreme}{théorème du rang}

        \[rangT + \dim \; \ker T  = \dim V\]
    
\end{theoreme}
    \begin{subparag}{Exemple}
        Considérons l'application linéaire $T: \mathbb{P}_2 \to $ \R$^2$ définie par:
        \[T\left(p\right) = \begin{pmatrix}
            p\left(0\right) \\ p\left(1\right)
        \end{pmatrix}\]
        Pour tout polynôme $p$ de degrès $\leq 2$.
        \\
        Nous allons 
        \begin{itemize}
            \item Calculer le noyau
            \item en déduire la dimension de l'image grâce au théorème du rang, et
            \item interpréter ce résultat pour conclure que deux zéros d'une fonction polynomiale de degrès $\leq 2$ déterminent le polynôme en question à un facteur près.
        \end{itemize}

        \begin{align*}
        \ker T &= \{p \in \mathbb{P}_2 | p\left(0\right) = 0 = p\left(1\right)\}\\
            &= \{p \in \mathbb{P}_2 | p\left(t\right) = a\cdot t\cdot \left(t - 1\right) \text{ , pour un } a \in \mathbb{R}\} \\
            &= Vect\{\left(t-1\right)\} \text{ de dimension 1.}
        \end{align*}
        Par le théorème du rang, $rang T + \dim\;\kerT = 3$ donc $rang T = 2 = \dim \mathbb{R}^2$.
        \\
        L'interprétation suit: si $p\left(t\right)$ s'annule en $0$ et en $1$, alors $p\left(t\right) = a \cdot t \cdot \left(t- 1\right)$ pour $a \in $ \R.
    \end{subparag}
\end{parag}
 \begin{parag}{Critère d'inversibilité}
     Soit $A$ une matrice carrée de taille $n\times n $. Alors les conditions suivantes sont équivalentes : 
     \begin{enumerate}
         \item La matrice $A$ est inversible
         \item les colonnes de $A$ forment une base de $\mathbb{R}^n$.
         \item $ImA = \mathbb{R}^n$ \hspace{5cm} $A$ est surjective
         \item $\dim \; Im A = n$.
         \item $rang\; A = n$
         \item $\ker\;A = \{0\}$ \hspace{4.75cm} $A$ est injective
         \item $\dim\; \ker\; A = 0$.
     \end{enumerate}
     \begin{subparag}{Exemple}
         Pour quelles valeurs du paramètre réel $a$ la matrice $A$ suivante est-elle inversible?
         \[\begin{pmatrix}
             a & 1 & 1 & 1\\
             1 & a & 1 & 1\\
             1 & 1 & a & 1\\
             1 & 1 & 1 & a
         \end{pmatrix} \to \begin{pmatrix}
            1 & 1 & 1 & a\\
             0 & a-1 & 0 & 1-a\\
             0 & 0 & a-1 & 1-a\\
             0 & 1-a & 1-a & 1-a^
             
         \end{pmatrix}\]
         (les opérations sont $L_1 - L-4$, $L-2 - L_1'$, $L_3  L_1'$, $L_4 - a\cdot L_1'$)
         Si $a - 1 = 0$ alors $a = 1$ donc:
         \\
         $rang\; A = 1$, $\dim \; \ker A = 3$
         \\
         Si $a \neq 1$, on peut diviser chaque ligne par $a-1$, on obtient alors:
 \[\begin{pmatrix}
            1 & 1 & 1 & a\\
             0 & 1 & 0 & -1\\
             0 & 0 & a-1 & -1\\
             0 & 1 & 1 & 1 + a
             
         \end{pmatrix} \to \begin{pmatrix}
            1 & 1 & 1 & a\\
             0 & 1 & 0 & -1\\
             0 & 0 & 1 & -1\\
             0 & 0 & 0 & a + 3
             
         \end{pmatrix}\]
         Les opération ici pour passer à la deuxième matrice sont ($L_3 - L_2 - L_3$) + $+$
     \end{subparag}
 \end{parag}

 \begin{parag}{Une application linéaire}
     Soit $W$ le sous-espace vectoriel de $M_{2\times2}\left(\mathbb{R}\right)$ des matrices triangulaires supérieures.
     \\
     En fait $W = Vect\{e_{11}, e_{12}, e_{22}$ et $\mathcal{B} = (e_{11}, e_{12}, e_{22})$ est une base de $W$.
     \\
     On considère $T : \mathbb{P}_2 \to W$ l'application linéaire définie par
     \[T\left(p\right) = \begin{pmatrix}
         p\left(1\right) & p\left(-1\right) \\
         0 & p\left(2\right)
     \end{pmatrix}\]
     En choisissant la base canonique $\mathcal{C}an = \left(1, t, t^2\right)$, on identifie $\mathbb{P}_2$ avec \R$^3$ et en choisissant la base ci-dessus de $W$ on identifie $W$ avec \R$^3$ également. Ce qui nous donne:
     \[\begin{pmatrix}
         1 \\ 0 \\ 0
     \end{pmatrix} = \vec{e}_1 = \left(1\right)_{\mathcal{C}an} \text{et} 1 \to \begin{pmatrix}
         1 & 1 \\ 0 & 1
     \end{pmatrix} = W_1 \text{et} \left(W_1\right)_{\mathcal{B}} = \begin{pmatrix}
         1 \\ 1 \\ 1
     \end{pmatrix}\]
     \[\begin{pmatrix}
         0 \\ 1 \\ 0
     \end{pmatrix} = \vec{e}_2 = \left(t\right)_{\mathcal{C}an} \text{et} t \to \begin{pmatrix}
         1 & -1 \\ 0 & 2
     \end{pmatrix} = W_1 \text{et} \left(W_2\right)_{\mathcal{B}} = \begin{pmatrix}
         1 \\ -1 \\ 2
     \end{pmatrix}\]
     \[\begin{pmatrix}
         0 \\ 0 \\ 1
     \end{pmatrix} = \vec{e}_3 = \left(t^2\right)_{\mathcal{C}an} \text{et} t^2 \to \begin{pmatrix}
         1 & 1 \\ 0 & 4
     \end{pmatrix} = W_3 \text{et} \left(W_3\right)_{\mathcal{B}} = \begin{pmatrix}
         1 \\ 1 \\ 4
     \end{pmatrix}\]
     \begin{subparag}{remarque}
         En gros on n'a juste "\textit{skip}" $a_{21}$ parce qu'il valait tout le temps $0$. Mais On aurait pu dire que l'ensemble $W$ est un sous-ensemble de $\mathbb{R}^4$ tel que $W \subset \mathbb{R}^4$.
     \end{subparag}
 \end{parag}

 \begin{parag}{Résumé}
     Le théroème du rang peut nous permettre de faire de grand raccourci lors de choix multiple.
     \begin{itemize}
         \item Le rang d'une matrice $A$ de taille $n \times m$ ne peut pas dépasser $n$.
         \[Rang  A + \dim\; \kerA = n\]
         \item Pour rappel, si on prend une application $A :$ $V \to W$ on a que $\ker A$ est dans $V$ et que $Im A$ est dans $W$.                               
     \end{itemize}
 \end{parag}